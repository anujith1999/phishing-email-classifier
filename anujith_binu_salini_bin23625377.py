# -*- coding: utf-8 -*-
"""ANUJITH BINU SALINI BIN23625377.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_bXAunNiI5YLGPqZK1awviIhaHjiZCvP

# MILESTONE 2

# Deep Learning for Email Phishing Detection with PyTorch
# Name: Anujith Binu Salini
# Student ID: BIN23625377

# Import Libraries

In this section, we import all the necessary libraries for data manipulation, deep learning, and using pre-trained models. We also load the dataset from Google Drive and print out the first few rows to ensure the data is correctly loaded.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertForSequenceClassification
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

"""# Load Dataset"""

# Mount Google Drive to Colab
from google.colab import drive
drive.mount('/content/drive')

# Load the dataset from Google Drive
file_path = '/content/drive/My Drive/Colab Notebooks/milestone2@/Phishing_Email.csv'

data = pd.read_csv(file_path)
print("Dataset Loaded Successfully!")
print(data.head())
print(data.info())

"""# Data Preprocessing

Here, we check for any missing values in the dataset using isnull().sum(). This step is crucial for ensuring the integrity of the data before proceeding with model training. If there are missing values, they need to be addressed (e.g., through imputation or removal).
"""

# Checking for missing data
print("Checking for missing values...")
print(data.isnull().sum())

texts = data['Email Type'].astype(str).values
labels = data['Email Type'].values
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(labels)

"""# Tokenization and Dataset Creation

In this section, we use the BertTokenizer to convert the email texts into tokenized input suitable for BERT. The EmailDataset class is a custom PyTorch dataset that handles tokenization and prepares the data for use in a deep learning model. Each email is tokenized into input IDs and attention masks, which BERT needs to understand the relevance of the tokens.
"""

# Tokenize the text data
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Custom Dataset class to handle tokenization
class EmailDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, item):
        text = str(self.texts[item])
        label = self.labels[item]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        input_ids = encoding['input_ids'].flatten()
        attention_mask = encoding['attention_mask'].flatten()

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': torch.tensor(label, dtype=torch.long)
        }

"""# Splitting Data into Training and Testing Sets

The dataset is split into training and testing sets (90% for training, 10% for testing)
 using train_test_split. We then create EmailDataset objects for both training and testing sets
 and use PyTorch's DataLoader to efficiently batch and shuffle the data for training.
"""

# Prepare train/test split
texts = data['Email Type'].values
# Create a new column 'Email Type Encoded' based on 'Email Type'
# Assuming 'Safe Email' is 0 and other types are 1
data['Email Type Encoded'] = data['Email Type'].apply(lambda x: 0 if x == 'Safe Email' else 1)
labels = data['Email Type Encoded'].values # Now this line should work

# Split data into training and testing sets (using the original validation split as test)
train_texts, test_texts, train_labels, test_labels = train_test_split(
    texts, labels, test_size=0.1, random_state=42
)

MAX_LEN = 64

# Create Dataset objects
train_dataset = EmailDataset(train_texts, train_labels, tokenizer, MAX_LEN)
test_dataset = EmailDataset(test_texts, test_labels, tokenizer, MAX_LEN)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# Custom Dataset class to handle tokenization
class EmailDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, item):
        text = str(self.texts[item])  # Convert text to string
        label = self.labels[item]  # Ensure label is numeric

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        input_ids = encoding['input_ids'].flatten()
        attention_mask = encoding['attention_mask'].flatten()

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': torch.tensor(label, dtype=torch.long)
        }

"""# Model Definition (Using Pretrained BERT)

Here, we define the deep learning model. We are using a pre-trained BERT model from the Hugging Face transformers library. This BERT model is fine-tuned for sequence classification, where the output logits represent the probability of the email being phishing or safe.
"""

# Using a pre-trained BERT model for sequence classification
class SimpleBERTClassifier(nn.Module):
    def __init__(self, num_classes=2):
        super(SimpleBERTClassifier, self).__init__()
        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.logits

import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize the model
model = SimpleBERTClassifier(num_classes=2).to(device)

"""# Define Optimizer and Loss Function

We define the optimizer and loss function here. We use the Adam optimizer (which is commonly used for training deep learning models) and the CrossEntropyLoss function, which is suitable for multi-class classification tasks like this one.
"""

# Define optimizer and loss function
optimizer = optim.Adam(model.parameters(), lr=2e-5)  # Learning rate can be adjusted
criterion = nn.CrossEntropyLoss()

"""# Train the Model

In the training loop, the model is trained for 10 epochs . For each epoch, the model is set to training mode, the optimizer gradients are zeroed, and the model makes predictions. The loss is computed and backpropagated, followed by updating the model's parameters. Training accuracy and loss are printed at the end of each epoch.
"""

# Training loop
def train_model(model, train_loader, val_loader, epochs=5):
    best_accuracy = 0
    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")
        model.train()
        running_loss = 0.0
        correct_preds = 0
        total_preds = 0

        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            correct_preds += (predicted == labels).sum().item()
            total_preds += labels.size(0)

        train_loss = running_loss / len(train_loader)
        train_accuracy = correct_preds / total_preds
        print(f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}")

        # Validation
        # The following lines were indented incorrectly, causing the IndentationError.
        # They should be inside the 'train_model' function and the epoch loop.
        model.eval()
        correct_preds = 0
        total_preds = 0
        val_loss = 0.0

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                outputs = model(input_ids, attention_mask)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                correct_preds += (predicted == labels).sum().item()
                total_preds += labels.size(0)

        val_loss /= len(val_loader)
        val_accuracy = correct_preds / total_preds
        print(f"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}")

        # Save model if accuracy improves
        if val_accuracy > best_accuracy:
            print("Saving model...")
            torch.save(model.state_dict(), '/content/drive/My Drive/Colab Notebooks/milestone2@/phishing_email_bert_model.pt')
            best_accuracy = val_accuracy

# Train the model
train_model(model, train_loader, test_loader, epochs=10)

"""# Evaluate the Model"""

# Evaluate the model on the validation set (using test_loader as val_loader)
model.eval()  # Set the model to evaluation mode
correct = 0
total = 0

with torch.no_grad():  # Disable gradient calculation for evaluation
    for batch in test_loader:  # Use test_loader here instead of val_loader
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids, attention_mask)
        _, predicted = torch.max(outputs, 1)
        correct += (predicted == labels).sum().item()
        total += labels.size(0)

val_accuracy = 100 * correct / total
print(f"Validation Accuracy: {val_accuracy:.2f}%")

print("Training complete. Model saved successfully.")

"""# MILESTONE 3

TESTING THE MODEL
"""

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Evaluate on dataset (test_loader)
all_labels = []
all_predictions = []

model.eval()
with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids, attention_mask)
        _, predicted = torch.max(outputs, 1)

        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Classification Report
print("Classification Report:")
print(classification_report(all_labels, all_predictions))

# Confusion Matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')

"""# Deploying Your Model as a Website"""

!pip install streamlit

import streamlit as st
from transformers import BertTokenizer, BertForSequenceClassification
import torch
import os
import torch.nn as nn # Import torch.nn as nn here

# Mount Google Drive to ensure access (if not already mounted)
from google.colab import drive
drive.mount('/content/drive')

# Define the path to the model file, using os.path.join for better compatibility across platforms
model_path = os.path.join('/content', 'drive', 'My Drive', 'Colab Notebooks', 'milestone2@', 'phishing_email_bert_model.pt')

# Define device here
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load the fine-tuned model
# Instead of loading from "phishing_email_model" (which isn't a standard model),
# load your saved model weights
# Initialize with the custom model class
class SimpleBERTClassifier(nn.Module):
    def __init__(self, num_classes=2):
        super(SimpleBERTClassifier, self).__init__()
        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.logits

model = SimpleBERTClassifier(num_classes=2)  # Initialize with your custom class

# Check if the model file exists before attempting to load it
if os.path.exists(model_path):
    model.load_state_dict(torch.load(model_path))  # Load your weights
    model.to(device)  # Move the model to the device (GPU if available)
else:
    st.error(f"Model file not found at: {model_path}. Please check the path and ensure the file exists.")
    st.stop()  # Stop execution if the model file is not found

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Streamlit UI
st.title("Phishing Email Classifier")
email_text = st.text_area("Enter the email content:")

# Prediction
if email_text:
    inputs = tokenizer(email_text, return_tensors="pt", truncation=True, padding=True).to(device) # Move inputs to the device
    outputs = model(inputs['input_ids'], inputs['attention_mask']) # Pass input_ids and attention_mask
    # logits = outputs.logits # No need for this line as SimpleBERTClassifier already returns logits
    prediction = torch.argmax(outputs, dim=-1).item()

    if prediction == 1:
        st.write("This is a phishing email.")
    else:
        st.write("This is not a phishing email.")